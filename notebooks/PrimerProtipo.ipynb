{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eed7acb",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Primer prototipo o pruebas con datos**\n",
    "\n",
    "En esta etapa necesitamos:\n",
    "\n",
    "1. **Cargar** una oraci√≥n de prueba.\n",
    "2. **Definir** manualmente los par√°metros del HMM (œÄ, A, B).\n",
    "3. **Preparar estructuras** para procesarlas en c√≥digo.\n",
    "\n",
    "### üìò TEOR√çA DE HMM (Modelo Oculto de Markov)\n",
    "\n",
    "Seg√∫n Rabiner:\n",
    "\n",
    "- Un HMM es un modelo estad√≠stico probabil√≠stico en el que se asume que el sistema que se modela es un proceso de Markov con estados ocultos. donde:\n",
    "    - El estado actual influye en los estados futuros, pero los estados en s√≠ no son directamente observables.\n",
    "    - Hay una secuencia de observaciones conocidas, lo que observamos son **emisiones** o **observaciones** que dependen del estado oculto actual.\n",
    "    - Hay una secuencia de **estados ocultos** desconocidos que queremos inferir.\n",
    "- Se compone de:\n",
    "    - **Conjunto de estados ocultos (S):** Un n√∫mero finito de estados que no son directamente observables. En el contexto del POS-tagging, estos estados representar√≠an las posibles etiquetas gramaticales (sustantivo, verbo, adjetivo, etc.).\n",
    "    - **Conjunto de observaciones (V):** Un n√∫mero finito de s√≠mbolos de observaci√≥n que son las salidas del sistema que podemos observar. En el POS-tagging, estas ser√≠an las palabras del texto que queremos etiquetar.\n",
    "- Se caracteriza por 3 conjuntos de par√°metros:\n",
    "    - **Matriz de probabilidades de transici√≥n de estados (A):** Define la probabilidad de transici√≥n entre los estados ocultos. $a_{ij} = P(q_{t+1} = S_j | q_t = S_i)$ es la probabilidad de estar en el estado $S_i$  en el tiempo $t$ y pasar al estado $S_j$ en el tiempo $t+1$.  ***(probabilidad de pasar de una etiqueta a otra)***\n",
    "    - **Matriz de probabilidades de emisi√≥n (B):** Define la probabilidad de observar un s√≠mbolo particular dado un estado oculto. $b_j(k) = P(O_t = v_k | q_t = S_j)$ es la probabilidad de observar el s√≠mbolo $v_k$ cuando el estado oculto en el tiempo $t$ es $S_j$. ***(probabilidad de que una palabra sea generada por una etiqueta.)***\n",
    "    - **Vector de probabilidades iniciales de estado (œÄ):** Define la probabilidad de estar en un estado particular en el tiempo inicial ($t=1$).\n",
    "        \n",
    "         $\\pi_i = P(q_1 = S_i)$ es la probabilidad de que el primer estado oculto sea $S_i$. ***(probabilidad de comenzar en cada etiqueta.)***\n",
    "        \n",
    "\n",
    "**HMM de Primer Orden**\n",
    "\n",
    "Un HMM de primer orden se caracteriza por la **propiedad de Markov de primer orden**, que establece que la probabilidad del estado actual solo depende del estado inmediatamente anterior. Matem√°ticamente, esto se expresa como:\n",
    "\n",
    "$P(q_t | q_{t-1}, q_{t-2}, ..., q_1) = P(q_t | q_{t-1})$\n",
    "\n",
    "De manera similar, la probabilidad de una observaci√≥n en un momento dado solo depende del estado oculto en ese mismo momento:\n",
    "\n",
    "$P(O_t | q_1, ..., q_t, O_1, ..., O_{t-1}) = P(O_t | q_t)$\n",
    "\n",
    "**HMM para POS-Tagging**\n",
    "\n",
    "En el contexto del POS-tagging, podemos mapear los componentes de un HMM de la siguiente manera:\n",
    "\n",
    "- **Estados ocultos (S):** El conjunto de posibles etiquetas gramaticales (e.g., {NOUN, VERB, ADJ, DET}).\n",
    "- **Observaciones (V):** El vocabulario del idioma (el conjunto de todas las palabras posibles).\n",
    "- **Probabilidades de transici√≥n (A):** La probabilidad de que una etiqueta gramatical siga a otra (e.g., la probabilidad de que un sustantivo siga a un determinante). Estas probabilidades se pueden estimar a partir de un corpus de texto etiquetado.\n",
    "- **Probabilidades de emisi√≥n (B):** La probabilidad de que una palabra particular sea generada por una etiqueta gramatical espec√≠fica (e.g., la probabilidad de que la palabra \"gato\" tenga la etiqueta NOUN). Estas probabilidades tambi√©n se pueden estimar a partir de un corpus de texto etiquetado.\n",
    "- **Probabilidades iniciales (œÄ):** La probabilidad de que una etiqueta gramatical sea la primera etiqueta en una oraci√≥n (e.g., la probabilidad de que la primera palabra de una oraci√≥n sea un determinante o un sustantivo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba2b27",
   "metadata": {},
   "source": [
    "### üß™ PRUEBA PR√ÅCTICA: Implementaci√≥n b√°sica del corpus \"Time flies like an arrow\"\n",
    "\n",
    "‚úÖ **¬øQu√© lograramos con esto?**\n",
    "\n",
    "- Armas el HMM desde cero.\n",
    "- El c√≥digo es totalmente controlado (no usa librer√≠as externas de NLP).\n",
    "- Prepara la base para aplicar **el algoritmo de Viterbi en el siguiente paso**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42efb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Oraci√≥n de prueba\n",
    "sentence = [\"Time\", \"flies\", \"like\", \"an\", \"arrow\"]\n",
    "\n",
    "# Estados posibles\n",
    "states = [\"NOUN\", \"VERB\", \"DET\", \"PREP\"]\n",
    "observations = sentence\n",
    "\n",
    "# √çndices\n",
    "state_idx = {s: i for i, s in enumerate(states)}\n",
    "obs_idx = {w: i for i, w in enumerate(observations)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88687fb4",
   "metadata": {},
   "source": [
    "### 1. üîµ œÄ ‚Äì Probabilidades iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f715a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# œÄ: Probabilidades iniciales\n",
    "pi = [0.4, 0.3, 0.2, 0.1]  # [NOUN, VERB, DET, PREP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a3ee5",
   "metadata": {},
   "source": [
    "### üìå ¬øQu√© representa?\n",
    "\n",
    "Es la **probabilidad de comenzar** una oraci√≥n con cada una de las etiquetas posibles. Por ejemplo:\n",
    "\n",
    "- ¬øQu√© tan probable es que la primera palabra de una oraci√≥n sea un **sustantivo** (`NOUN`)? Bastante com√∫n ‚Üí 0.4\n",
    "- ¬øUn verbo al inicio? Posible pero no tanto ‚Üí 0.3\n",
    "- ¬øUn determinante (`DET`)? Como \"The\", \"An\" ‚Üí 0.2\n",
    "- ¬øUna preposici√≥n (`PREP`)? Muy raro ‚Üí 0.1\n",
    "\n",
    "### üìã Criterio usado:\n",
    "\n",
    "- Uso **intuitivo / heur√≠stico** basado en c√≥mo suelen comenzar las oraciones en ingl√©s.\n",
    "- No usamos un corpus grande para entrenarlo (a√∫n), as√≠ que lo **asignamos a mano de forma razonada**.\n",
    "\n",
    "### 2. üîÅ `A` ‚Äì Matriz de transici√≥n de estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8b1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: Matriz de transici√≥n\n",
    "A = np.array([\n",
    "    [0.1, 0.6, 0.2, 0.1],  # desde NOUN\n",
    "    [0.3, 0.1, 0.1, 0.5],  # desde VERB\n",
    "    [0.7, 0.1, 0.1, 0.1],  # desde DET\n",
    "    [0.6, 0.2, 0.1, 0.1],  # desde PREP\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f37a3a",
   "metadata": {},
   "source": [
    "### üìå ¬øQu√© representa?\n",
    "\n",
    "Cada fila dice: \"si estoy en la etiqueta X, ¬øa qu√© etiqueta paso despu√©s con qu√© probabilidad?\"\n",
    "\n",
    "### Ejemplos:\n",
    "\n",
    "- Desde `DET`, la probabilidad de pasar a `NOUN` es alta ‚Üí 0.7 (como en \"an arrow\").\n",
    "- Desde `NOUN`, es com√∫n que siga un verbo ‚Üí 0.6\n",
    "- Desde `VERB`, es com√∫n que venga una preposici√≥n ‚Üí 0.5 (como en \"like an arrow\").\n",
    "\n",
    "### üìã Criterio usado:\n",
    "\n",
    "- Heur√≠stica inspirada en reglas gramaticales t√≠picas del ingl√©s.\n",
    "- Usamos **conocimiento ling√º√≠stico general** (y un poco de l√≥gica narrativa) para simular lo que har√≠a un corpus entrenado.\n",
    "\n",
    "### 3. üü£ `B` ‚Äì Matriz de emisi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968e33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B: Matriz de emisi√≥n\n",
    "B = np.array([\n",
    "    [0.3, 0.2, 0.1, 0.0, 0.4],  # NOUN\n",
    "    [0.1, 0.6, 0.2, 0.0, 0.1],  # VERB\n",
    "    [0.0, 0.0, 0.0, 1.0, 0.0],  # DET\n",
    "    [0.0, 0.0, 0.9, 0.1, 0.0],  # PREP\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e1a2f",
   "metadata": {},
   "source": [
    "### üìå ¬øQu√© representa?\n",
    "\n",
    "Cada fila te dice: \"¬øcu√°l es la probabilidad de que la palabra *X* haya sido generada por esta etiqueta?\"\n",
    "\n",
    "### Ejemplos:\n",
    "\n",
    "- `\"flies\"` ‚Üí puede ser `VERB` (0.6) o `NOUN` (0.2) ‚Üí ¬°ambig√ºedad cl√°sica!\n",
    "- `\"like\"` ‚Üí puede ser `PREP` (0.9) o `VERB` (0.2)\n",
    "- `\"an\"` ‚Üí clar√≠simo que es `DET` (1.0)\n",
    "- `\"arrow\"` ‚Üí muy probablemente un `NOUN` (0.4)\n",
    "\n",
    "### üìã Criterio usado:\n",
    "\n",
    "- Analizamos sem√°ntica y **ambig√ºedad l√©xica** de cada palabra.\n",
    "- Los valores se asignan **a mano**, con la idea de **reflejar ambig√ºedad realista**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb513f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estados: ['NOUN', 'VERB', 'DET', 'PREP']\n",
      "Observaciones: ['Time', 'flies', 'like', 'an', 'arrow']\n",
      "œÄ: [0.4, 0.3, 0.2, 0.1]\n",
      "A:\n",
      "       NOUN  VERB  DET  PREP\n",
      "NOUN   0.1   0.6  0.2   0.1\n",
      "VERB   0.3   0.1  0.1   0.5\n",
      "DET    0.7   0.1  0.1   0.1\n",
      "PREP   0.6   0.2  0.1   0.1\n",
      "B:\n",
      "       Time  flies  like   an  arrow\n",
      "NOUN   0.3    0.2   0.1  0.0    0.4\n",
      "VERB   0.1    0.6   0.2  0.0    0.1\n",
      "DET    0.0    0.0   0.0  1.0    0.0\n",
      "PREP   0.0    0.0   0.9  0.1    0.0\n"
     ]
    }
   ],
   "source": [
    "# Mostrar matrices\n",
    "print(\"Estados:\", states)\n",
    "print(\"Observaciones:\", observations)\n",
    "print(\"œÄ:\", pi)\n",
    "print(\"A:\\n\", pd.DataFrame(A, index=states, columns=states))\n",
    "print(\"B:\\n\", pd.DataFrame(B, index=states, columns=observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b1680",
   "metadata": {},
   "source": [
    "## ¬øPor qu√© no usamos datos reales?\n",
    "\n",
    "Porque estamos en una **fase inicial/prototipo** donde queremos entender bien c√≥mo funciona el modelo. Estos valores nos permiten:\n",
    "\n",
    "- Testear el algoritmo de Viterbi sin necesidad de entrenamiento.\n",
    "- Ver c√≥mo cambia la secuencia etiquetada si alteramos las probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427838",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
