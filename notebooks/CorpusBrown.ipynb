{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ ¬øQu√© haremos en esta parte?\n",
        "\n",
        "### **Objetivo**:\n",
        "\n",
        "Usar el **Brown Corpus** (un corpus etiquetado del ingl√©s) para:\n",
        "\n",
        "- Probar el algoritmo de Viterbi.\n",
        "- Validar los resultados en una oraci√≥n real etiquetada.\n",
        "- Comparar la secuencia etiquetada con la secuencia ‚Äúverdadera‚Äù.\n",
        "\n",
        "## üîß PASOS PARA AVANZAR:\n",
        "\n",
        "### üì¶ Paso 1: Instalar y cargar el corpus Brown\n",
        "\n",
        "Usaremos **NLTK**, una librer√≠a muy com√∫n para NLP en Python.\n",
        "\n",
        "```bash\n",
        "pip install nltk\n",
        "```\n",
        "\n",
        "C√≥digo para cargar una parte del corpus:"
      ],
      "metadata": {
        "id": "LPdpJlJF_lDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3lXVJ62j07Z",
        "outputId": "2d1a534a-5346-4a21-c269-862b6bc7c3d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicacion del codigo:\n",
        "\n",
        "- Se importa la biblioteca **NLTK (Natural Language Toolkit)**, una de las bibliotecas m√°s utilizadas en Python para procesamiento de lenguaje natural. Contiene herramientas para tareas como tokenizaci√≥n, etiquetado gramatical (POS tagging), an√°lisis sint√°ctico, etc.\n",
        "- Se descarga el **Brown Corpus**, uno de los corpus m√°s antiguos y ampliamente utilizados en ling√º√≠stica computacional. Es una colecci√≥n de textos en ingl√©s estadounidense divididos en categor√≠as tem√°ticas. Se usa para entrenar y probar modelos de lenguaje, an√°lisis gramatical, etc.\n",
        "- Se descarga el **Universal Tagset**, que es un conjunto simplificado y estandarizado de etiquetas gramaticales (como `NOUN`, `VERB`, `ADJ`, etc.). Sirve para facilitar comparaciones entre lenguajes o corpus que utilizan diferentes conjuntos de etiquetas.\n",
        "\n",
        "\n",
        "‚úÖ **Resumen Visual:**\n",
        "\n",
        "```\n",
        "[NLTK] ---> [Brown Corpus üìò] + [Universal POS Tags üè∑Ô∏è]\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Luego, cargamos las oraciones etiquetadas con un **tagset universal** (simplificado, como NOUN, VERB, etc.):"
      ],
      "metadata": {
        "id": "oLUJx19PAHRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "# Obtener oraciones etiquetadas (solo las 5000 primeras para empezar)\n",
        "tagged_sents = brown.tagged_sents(tagset='universal')[:5000]\n",
        "tagged_sents[:1] # Visualizacion de la primera oracion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUCuW-kXj8QU",
        "outputId": "fe2ccf7c-775f-4c01-8d09-7a0fdd0ac209"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'DET'),\n",
              "  ('Fulton', 'NOUN'),\n",
              "  ('County', 'NOUN'),\n",
              "  ('Grand', 'ADJ'),\n",
              "  ('Jury', 'NOUN'),\n",
              "  ('said', 'VERB'),\n",
              "  ('Friday', 'NOUN'),\n",
              "  ('an', 'DET'),\n",
              "  ('investigation', 'NOUN'),\n",
              "  ('of', 'ADP'),\n",
              "  (\"Atlanta's\", 'NOUN'),\n",
              "  ('recent', 'ADJ'),\n",
              "  ('primary', 'NOUN'),\n",
              "  ('election', 'NOUN'),\n",
              "  ('produced', 'VERB'),\n",
              "  ('``', '.'),\n",
              "  ('no', 'DET'),\n",
              "  ('evidence', 'NOUN'),\n",
              "  (\"''\", '.'),\n",
              "  ('that', 'ADP'),\n",
              "  ('any', 'DET'),\n",
              "  ('irregularities', 'NOUN'),\n",
              "  ('took', 'VERB'),\n",
              "  ('place', 'NOUN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicacion del codigo:\n",
        "\n",
        "- Se importa el **corpus Brown** desde el m√≥dulo `nltk.corpus`. Esto nos da acceso directo al contenido del corpus, como palabras, oraciones y sus etiquetas gramaticales.\n",
        "- `brown.tagged_sents(tagset='universal')`:\n",
        "    - Esto obtiene las oraciones del corpus **Brown**, donde cada palabra est√° **etiquetada gramaticalmente** (por ejemplo: 'the/DET', 'dog/NOUN').\n",
        "    - El par√°metro `tagset='universal'` convierte las etiquetas del corpus a un conjunto **simplificado y estandarizado** de etiquetas gramaticales universales (como `NOUN`, `VERB`, `ADP`, etc.), en lugar del conjunto original del Brown Corpus.\n",
        "---\n",
        "\n",
        "### üß† Paso 2: Extraer listas de etiquetas y palabras\n",
        "\n",
        "Vamos a obtener todas las **etiquetas** y **palabras** del corpus y crear un **vocabulario y conjunto de etiquetas √∫nicos**:"
      ],
      "metadata": {
        "id": "cWT8d7-BAVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listas vac√≠as para almacenar\n",
        "tag_sequences = []\n",
        "word_sequences = []\n",
        "\n",
        "for sent in tagged_sents:\n",
        "    words, tags = zip(*sent)\n",
        "    word_sequences.append(list(words))\n",
        "    tag_sequences.append(list(tags))\n",
        "\n",
        "print(\"Word despues del Zip\", words)\n",
        "print(\"Tags despues del Zip\", tags)\n",
        "print(\"Word secuence\", word_sequences[-2:])\n",
        "print(\"Tag secuence\", tag_sequences[-2:])\n",
        "\n",
        "\n",
        "# Vocabulario de palabras y etiquetas\n",
        "all_tags = sorted(set(tag for tags in tag_sequences for tag in tags))\n",
        "all_words = sorted(set(word.lower() for words in word_sequences for word in words))\n",
        "\n",
        "# Crear √≠ndices\n",
        "tag2idx = {tag: i for i, tag in enumerate(all_tags)}\n",
        "word2idx = {word: i for i, word in enumerate(all_words)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbpUa5cLL3x4",
        "outputId": "8805d484-40be-4267-86c6-a27c0596dbd7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word despues del Zip ('Does', 'it', 'attack', 'other', 'traditional', 'American', 'institutions', 'with', 'unsupportable', 'and', 'wild', 'charges', '?', '?')\n",
            "Tags despues del Zip ('VERB', 'PRON', 'VERB', 'ADJ', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'CONJ', 'ADJ', 'NOUN', '.', '.')\n",
            "Word secuence [['2', '.'], ['Does', 'it', 'attack', 'other', 'traditional', 'American', 'institutions', 'with', 'unsupportable', 'and', 'wild', 'charges', '?', '?']]\n",
            "Tag secuence [['NUM', '.'], ['VERB', 'PRON', 'VERB', 'ADJ', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'CONJ', 'ADJ', 'NOUN', '.', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Todas las palabras\", all_words[900:920])\n",
        "print(\"Todas las etiquetas\", all_tags)\n",
        "print(\"Indices de palabras\", dict(list(word2idx.items())[900:920]))\n",
        "print(\"Indices de etiquetas\", tag2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78kZnwgKWcAh",
        "outputId": "a8ab216a-8f9f-4feb-d2e5-bba70fd3be75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todas las palabras ['accused', 'ace', 'achaeans', \"achaeans'\", 'achieve', 'achieved', 'achievement', 'achievements', 'achieves', 'aching', 'acid', 'acknowledge', 'acknowledged', 'acknowledging', 'acknowledgment', 'acquaint', 'acquaintance', 'acquiesce', 'acquiesced', 'acquire']\n",
            "Todas las etiquetas ['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n",
            "Indices de palabras {'accused': 900, 'ace': 901, 'achaeans': 902, \"achaeans'\": 903, 'achieve': 904, 'achieved': 905, 'achievement': 906, 'achievements': 907, 'achieves': 908, 'aching': 909, 'acid': 910, 'acknowledge': 911, 'acknowledged': 912, 'acknowledging': 913, 'acknowledgment': 914, 'acquaint': 915, 'acquaintance': 916, 'acquiesce': 917, 'acquiesced': 918, 'acquire': 919}\n",
            "Indices de etiquetas {'.': 0, 'ADJ': 1, 'ADP': 2, 'ADV': 3, 'CONJ': 4, 'DET': 5, 'NOUN': 6, 'NUM': 7, 'PRON': 8, 'PRT': 9, 'VERB': 10, 'X': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicacion del codigo:\n",
        "\n",
        "| Parte | Descripci√≥n |\n",
        "| --- | --- |\n",
        "| `tag_sequences`, `word_sequences` | üìÑ Listas para guardar solo las palabras y etiquetas de cada oraci√≥n |\n",
        "| `zip(*sent)` | Separa cada oraci√≥n etiquetada en dos listas: palabras y etiquetas |\n",
        "| `set(... for ...)` | Extrae todas las **palabras √∫nicas** (en min√∫scula) y **etiquetas √∫nicas** |\n",
        "| `enumerate(...)` | Asigna un **√≠ndice num√©rico** a cada palabra y etiqueta |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Resumen visual paso a paso:**\n",
        "\n",
        "```\n",
        "1. [('The', 'DET'), ('dog', 'NOUN')] ‚Üí zip ‚Üí ['The', 'dog'], ['DET', 'NOUN']\n",
        "2. word_sequences = [['The', 'dog'], ...]\n",
        "3. tag_sequences  = [['DET', 'NOUN'], ...]\n",
        "\n",
        "4. all_words = ['a', 'about', 'accident', ...]  ‚Üí word2idx = {'a': 0, 'about': 1, ...}\n",
        "5. all_tags  = ['ADJ', 'ADV', 'DET', ...]      ‚Üí tag2idx  = {'ADJ': 0, 'ADV': 1, ...}\n",
        "```\n",
        "---\n",
        "\n",
        "## üéØ Paso 3: Estimar œÄ (probabilidades iniciales)\n",
        "\n",
        "La probabilidad œÄ representa qu√© **tan seguido cada etiqueta inicia una oraci√≥n**:"
      ],
      "metadata": {
        "id": "5PH1AeK7RNS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_tags = len(all_tags)\n",
        "pi = np.zeros(num_tags)\n",
        "\n",
        "# Contar cu√°ntas veces comienza una oraci√≥n con cada etiqueta\n",
        "for tags in tag_sequences:\n",
        "    first_tag = tags[0]\n",
        "    pi[tag2idx[first_tag]] += 1\n",
        "\n",
        "# Normalizar\n",
        "pi /= pi.sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "kmhqOtakRglD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimire pi mostrando a que etiqueta corresponde esa probabilida y cual fue su conteo\n",
        "for i, p in enumerate(pi):\n",
        "    print(f\"{all_tags[i]}: {p:.4f} ({pi[i] * len(tag_sequences):.0f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se-i5gvaeTG6",
        "outputId": "85a5c1b0-97ae-46a1-b428-6f0bf51e13cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".: 0.0824 (412)\n",
            "ADJ: 0.0460 (230)\n",
            "ADP: 0.1006 (503)\n",
            "ADV: 0.0578 (289)\n",
            "CONJ: 0.0380 (190)\n",
            "DET: 0.2470 (1235)\n",
            "NOUN: 0.2520 (1260)\n",
            "NUM: 0.0164 (82)\n",
            "PRON: 0.0952 (476)\n",
            "PRT: 0.0272 (136)\n",
            "VERB: 0.0368 (184)\n",
            "X: 0.0006 (3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicacion del codigo:\n",
        "\n",
        "### Vector de estados iniciales `œÄ`\n",
        "\n",
        "```python\n",
        "num_tags = len(all_tags)     # Total de etiquetas (estados)\n",
        "pi = np.zeros(num_tags)      # Inicializa un vector de ceros\n",
        "```\n",
        "\n",
        "Se crea un vector llamado `pi` que guardar√° **la probabilidad de comenzar una oraci√≥n con cada etiqueta gramatical**.\n",
        "\n",
        "Visualmente ser√≠a algo as√≠ como:\n",
        "\n",
        "```\n",
        "pi = [0, 0, 0, 0, ...]  ‚Üê uno por cada etiqueta (NOUN, VERB, DET, etc.)\n",
        "```\n",
        "\n",
        "### Contar etiquetas iniciales\n",
        "\n",
        "```python\n",
        "for tags in tag_sequences:\n",
        "    first_tag = tags[0]\n",
        "    pi[tag2idx[first_tag]] += 1\n",
        "```\n",
        "\n",
        "Recorre todas las oraciones y **cuenta con qu√© etiqueta comienza cada una**.\n",
        "\n",
        "Ejemplo\n",
        "\n",
        "- Si una oraci√≥n empieza con `DET`, suma 1 al √≠ndice correspondiente en `pi`.\n",
        "\n",
        "### Normalizaci√≥n\n",
        "\n",
        "```python\n",
        "pi /= pi.sum()\n",
        "```\n",
        "\n",
        "Convierte los conteos en **probabilidades reales** (todas suman 1).\n",
        "\n",
        "üìä Ejemplo conceptual:\n",
        "\n",
        "| Etiqueta inicial | Conteo | Probabilidad (œÄ) |\n",
        "| --- | --- | --- |\n",
        "| DET | 2000 | 0.40 |\n",
        "| NOUN | 1500 | 0.30 |\n",
        "| VERB | 1500 | 0.30 |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Paso 4: Estimar A (matriz de transici√≥n)\n",
        "\n",
        "La matriz A cuenta la frecuencia con que una etiqueta es seguida por otra:"
      ],
      "metadata": {
        "id": "A7bEnzsBdoLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "for tags in tag_sequences:\n",
        "    for i in range(1, len(tags)):\n",
        "        prev_tag = tags[i-1]\n",
        "        curr_tag = tags[i]\n",
        "        A[tag2idx[prev_tag], tag2idx[curr_tag]] += 1\n",
        "\n",
        "# Normalizar por filas (para que cada fila sume 1)\n",
        "A = A / A.sum(axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "G51-w0eCd2oa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#imprimire la matriz A donde las columas y filas tengan de nombre las etiquetas\n",
        "print(pd.DataFrame(A, index=all_tags, columns=all_tags).round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0ECtdQYfmE3",
        "outputId": "a4be90a1-a60b-4b6a-bcff-8361f460e74f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          .    ADJ    ADP    ADV   CONJ    DET   NOUN    NUM   PRON    PRT  \\\n",
            ".     0.126  0.043  0.107  0.052  0.080  0.120  0.216  0.029  0.067  0.022   \n",
            "ADJ   0.066  0.062  0.073  0.005  0.026  0.005  0.711  0.018  0.002  0.016   \n",
            "ADP   0.008  0.079  0.017  0.011  0.001  0.440  0.304  0.055  0.035  0.009   \n",
            "ADV   0.129  0.123  0.153  0.079  0.017  0.085  0.053  0.023  0.036  0.029   \n",
            "CONJ  0.018  0.112  0.061  0.059  0.000  0.147  0.336  0.025  0.041  0.025   \n",
            "DET   0.010  0.236  0.008  0.014  0.000  0.006  0.643  0.018  0.006  0.002   \n",
            "NOUN  0.251  0.017  0.216  0.021  0.049  0.012  0.254  0.010  0.013  0.017   \n",
            "NUM   0.230  0.071  0.136  0.038  0.031  0.009  0.413  0.015  0.004  0.007   \n",
            "PRON  0.066  0.010  0.046  0.056  0.012  0.013  0.007  0.001  0.008  0.020   \n",
            "PRT   0.040  0.018  0.098  0.031  0.008  0.081  0.041  0.012  0.002  0.009   \n",
            "VERB  0.065  0.052  0.172  0.076  0.010  0.179  0.126  0.017  0.032  0.066   \n",
            "X     0.212  0.000  0.071  0.010  0.010  0.000  0.131  0.000  0.000  0.010   \n",
            "\n",
            "       VERB      X  \n",
            ".     0.137  0.001  \n",
            "ADJ   0.016  0.000  \n",
            "ADP   0.039  0.000  \n",
            "ADV   0.274  0.000  \n",
            "CONJ  0.175  0.000  \n",
            "DET   0.056  0.001  \n",
            "NOUN  0.140  0.000  \n",
            "NUM   0.047  0.000  \n",
            "PRON  0.761  0.000  \n",
            "PRT   0.659  0.000  \n",
            "VERB  0.204  0.000  \n",
            "X     0.020  0.535  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicacion del codigo:\n",
        "\n",
        "```python\n",
        "A = np.zeros((num_tags, num_tags))\n",
        "```\n",
        "\n",
        "Crea una **matriz de ceros** de tama√±o `(n√∫mero de etiquetas x n√∫mero de etiquetas)`, es decir, una cuadr√≠cula donde se guardar√°n las **probabilidades de transici√≥n entre etiquetas gramaticales**.\n",
        "\n",
        "### Recorriendo secuencias para contar transiciones\n",
        "\n",
        "```python\n",
        "for tags in tag_sequences:\n",
        "    for i in range(1, len(tags)):\n",
        "        prev_tag = tags[i-1]\n",
        "        curr_tag = tags[i]\n",
        "        A[tag2idx[prev_tag], tag2idx[curr_tag]] += 1\n",
        "```\n",
        "\n",
        "üß© Por cada oraci√≥n, compara **pares consecutivos de etiquetas** como:\n",
        "\n",
        "```python\n",
        "['DET', 'NOUN', 'VERB']\n",
        "```\n",
        "\n",
        "Esto genera transiciones:\n",
        "\n",
        "- `DET ‚Üí NOUN`\n",
        "- `NOUN ‚Üí VERB`\n",
        "\n",
        "Y suma 1 en la celda correspondiente de la matriz `A`.\n",
        "\n",
        "Visualmente:\n",
        "\n",
        "| De \\ A | DET | NOUN | VERB |\n",
        "| --- | --- | --- | --- |\n",
        "| **DET** | 0 | 10 | 2 |\n",
        "| **NOUN** | 1 | 3 | 8 |\n",
        "| **VERB** | 2 | 1 | 0 |\n",
        "\n",
        "### üß™ Normalizar filas\n",
        "\n",
        "```python\n",
        "A = A / A.sum(axis=1, keepdims=True)\n",
        "```\n",
        "\n",
        "Convierte los conteos en **probabilidades**, de modo que cada fila (transiciones desde una etiqueta) sume 1.\n",
        "\n",
        "As√≠, por ejemplo:\n",
        "\n",
        "```\n",
        "A[tag2idx['DET']] = [0.05, 0.85, 0.10]\n",
        "```\n",
        "\n",
        "Significa:\n",
        "\n",
        "üìå Si est√°s en `DET`, hay **85% de chance de ir a NOUN**, 10% a VERB, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Paso 5: Estimar B (matriz de emisi√≥n)\n",
        "\n",
        "La matriz B cuenta la frecuencia con que una palabra se observa bajo una etiqueta:"
      ],
      "metadata": {
        "id": "p_uUw75ofWug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(all_words)\n",
        "B = np.zeros((num_tags, num_words))\n",
        "\n",
        "for words, tags in zip(word_sequences, tag_sequences):\n",
        "    for word, tag in zip(words, tags):\n",
        "        w = word.lower()\n",
        "        B[tag2idx[tag], word2idx[w]] += 1\n",
        "\n",
        "# Normalizar por filas (cada fila suma 1)\n",
        "B = B / B.sum(axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "4tGHcUzYfVNA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imrprimire B para que se visualice en las filas las etiquetas y en las columnas las palabras\n",
        "\n",
        "print(pd.DataFrame(B, index=all_tags, columns=all_words).round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyZYQsbyhakb",
        "outputId": "3f2a520e-ec52-4cd9-d949-83c20d1a7662"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          !   $1  $1,000  $1,000,000  $1,000,000,000  $1,250,000  $1,500  \\\n",
            ".     0.003  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "ADJ   0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "ADP   0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "ADV   0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "CONJ  0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "DET   0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "NOUN  0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "NUM   0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "PRON  0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "PRT   0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "VERB  0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "X     0.000  0.0     0.0         0.0             0.0         0.0     0.0   \n",
            "\n",
            "      $1,500,000  $1,600  $1,750,000  ...  zinman  zoe  zombies  zone  zones  \\\n",
            ".            0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "ADJ          0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "ADP          0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "ADV          0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "CONJ         0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "DET          0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "NOUN         0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "NUM          0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "PRON         0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "PRT          0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "VERB         0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "X            0.0     0.0         0.0  ...     0.0  0.0      0.0   0.0    0.0   \n",
            "\n",
            "      zoning  zoo  zubkovskaya  zurcher  zurich  \n",
            ".        0.0  0.0          0.0      0.0     0.0  \n",
            "ADJ      0.0  0.0          0.0      0.0     0.0  \n",
            "ADP      0.0  0.0          0.0      0.0     0.0  \n",
            "ADV      0.0  0.0          0.0      0.0     0.0  \n",
            "CONJ     0.0  0.0          0.0      0.0     0.0  \n",
            "DET      0.0  0.0          0.0      0.0     0.0  \n",
            "NOUN     0.0  0.0          0.0      0.0     0.0  \n",
            "NUM      0.0  0.0          0.0      0.0     0.0  \n",
            "PRON     0.0  0.0          0.0      0.0     0.0  \n",
            "PRT      0.0  0.0          0.0      0.0     0.0  \n",
            "VERB     0.0  0.0          0.0      0.0     0.0  \n",
            "X        0.0  0.0          0.0      0.0     0.0  \n",
            "\n",
            "[12 rows x 13735 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicacion del codigo:\n",
        "\n",
        "```python\n",
        "num_words = len(all_words)\n",
        "B = np.zeros((num_tags, num_words))\n",
        "```\n",
        "\n",
        "Crea una **matriz de emisi√≥n** `B`, de tama√±o:\n",
        "\n",
        "```\n",
        "(num_etiquetas x num_palabras)\n",
        "```\n",
        "\n",
        "Cada celda representar√°:\n",
        "\n",
        "> ¬øCon qu√© probabilidad una etiqueta (como NOUN) \"emite\" una palabra (como dog)?\n",
        ">\n",
        "\n",
        "### Recorriendo palabras y etiquetas\n",
        "\n",
        "```python\n",
        "for words, tags in zip(word_sequences, tag_sequences):\n",
        "    for word, tag in zip(words, tags):\n",
        "        w = word.lower()\n",
        "        B[tag2idx[tag], word2idx[w]] += 1\n",
        "```\n",
        "\n",
        "Va palabra por palabra, etiqueta por etiqueta, y suma 1 en la celda correspondiente de la matriz `B`.\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "- Palabra: `\"runs\"`\n",
        "- Etiqueta: `\"VERB\"`\n",
        "    \n",
        "    ‚Üí Suma 1 en: `B[VERB, runs]`\n",
        "    \n",
        "\n",
        "### Normalizaci√≥n por filas\n",
        "\n",
        "```python\n",
        "B = B / B.sum(axis=1, keepdims=True)\n",
        "```\n",
        "\n",
        "Convierte los conteos en **probabilidades reales**.\n",
        "\n",
        "Cada fila representa una etiqueta (`NOUN`, `VERB`, etc.) y **suma 1**.\n",
        "\n",
        "Ejemplo visual:\n",
        "\n",
        "| Etiqueta (fila) | ... 'dog' | 'runs' | 'quickly' | ... |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| **NOUN** | 0.045 | 0.001 | 0.000 | ... |\n",
        "| **VERB** | 0.000 | 0.080 | 0.002 | ... |\n",
        "| **ADV** | 0.000 | 0.000 | 0.140 | ... |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Listo! Ya tienemos œÄ, A y B a partir del Brown Corpus\n",
        "\n",
        "Estos tres componentes ahora nos permiten:\n",
        "\n",
        "- Usar **el algoritmo de Viterbi con datos reales**.\n",
        "- Evaluar frases reales del corpus.\n",
        "- Hacer comparaci√≥n entre la etiqueta predicha y la verdadera."
      ],
      "metadata": {
        "id": "cnsUEF4kg4b6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üöÄ **Paso 1: Seleccionar una oraci√≥n del Brown Corpus**\n",
        "\n",
        "Vamos a seleccionar una oraci√≥n del corpus, procesarla y aplicar el algoritmo de Viterbi para obtener la secuencia de etiquetas m√°s probable.\n",
        "\n",
        "Primero, seleccionamos una oraci√≥n del corpus:"
      ],
      "metadata": {
        "id": "fkXZQMcV9b8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar la primera oraci√≥n del Brown Corpus (etiquetada)\n",
        "sentence = brown.sents()[0]  # Primer oraci√≥n del corpus\n",
        "tagged_sentence = brown.tagged_sents(tagset='universal')[0]  # Primer oraci√≥n etiquetada\n",
        "print(\"Oraci√≥n original:\", ' '.join(sentence))\n",
        "print(\"Oraci√≥n etiquetada:\", tagged_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv6fjZMM9bMe",
        "outputId": "a430b9bf-fe67-42d8-8cb5-04fe291795aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oraci√≥n original: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
            "Oraci√≥n etiquetada: [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßë‚Äçüíª **Paso 2: Aplicar el algoritmo de Viterbi**\n",
        "\n",
        "Ahora que tenemos las matrices œÄ, A y B, aplicamos el algoritmo de **Viterbi** que ya implementamos, pero esta vez con una **oraci√≥n del corpus real**.\n",
        "\n",
        "### Parte 1: **Preparaci√≥n de los Datos**"
      ],
      "metadata": {
        "id": "DqVO3rqI-tt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir las palabras de la oraci√≥n a √≠ndices\n",
        "obs = [word.lower() for word in sentence]  # Convertir palabras a min√∫sculas\n",
        "obs_idx = [word2idx[word] if word in word2idx else 0 for word in obs]  # Mapeo a √≠ndices"
      ],
      "metadata": {
        "id": "OOjhsa1Q-wvr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Convertir las palabras de la oraci√≥n a min√∫sculas**:\n",
        "    - Aqu√≠, estamos tomando la **oraci√≥n original** (que es una lista de palabras) y convirtiendo cada palabra a **min√∫sculas** para normalizar la entrada. Esto asegura que \"Perro\" y \"perro\" se traten igual.\n",
        "2. **Mapear las palabras a √≠ndices**:\n",
        "    - convertimos cada palabra de la oraci√≥n **`obs`** a su √≠ndice correspondiente en el vocabulario utilizando el diccionario `word2idx`. Si una palabra no est√° en el vocabulario (por ejemplo, una palabra desconocida o fuera del conjunto de entrenamiento), se asigna el √≠ndice 0 (generalmente usado como \"palabra desconocida\").\n",
        "\n",
        "### Parte 2: **Inicializaci√≥n de Variables**"
      ],
      "metadata": {
        "id": "6Yy0wqAj-4ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N√∫mero de estados y palabras\n",
        "T = len(obs)\n",
        "N = len(all_tags)\n",
        "\n",
        "# Inicializar matrices delta y backpointers\n",
        "delta = np.zeros((T, N))\n",
        "psi = np.zeros((T, N), dtype=int)"
      ],
      "metadata": {
        "id": "DkFZ4dnz-6C0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **N√∫mero de estados (N) y observaciones (T)**:\n",
        "    - **`T`** es el n√∫mero de palabras (o **observaciones**) en la oraci√≥n. Esto se obtiene a partir de la longitud de la lista `obs`.\n",
        "    - **`N`** es el n√∫mero de posibles **etiquetas** (o **estados**) que puede tener nuestra secuencia, que es igual al n√∫mero de etiquetas en `all_tags`.\n",
        "2. **Inicializaci√≥n de las matrices delta y psi**:\n",
        "    - **`delta`** es una matriz que almacenar√° las probabilidades de las secuencias de estados m√°s probables hasta el momento (una especie de \"probabilidad acumulada\").\n",
        "    - **`psi`** es una matriz de **backpointers** que nos ayudar√° a realizar el retroceso (retroceder en el tiempo para encontrar la mejor secuencia de estados).\n",
        "\n",
        "### Parte 3: **Inicializaci√≥n de las Matrices (Paso Base)**"
      ],
      "metadata": {
        "id": "2V2YgOh1-90E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializaci√≥n\n",
        "for s in range(N):\n",
        "    delta[0, s] = pi[s] * B[s, obs_idx[0]]\n",
        "    psi[0, s] = 0  # No hay backpointer en el primer paso"
      ],
      "metadata": {
        "id": "94FnxoDX_Apw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Inicializaci√≥n de `delta` y `psi` para el primer estado**:\n",
        "- Para el primer estado (**t = 0**), se calcula la probabilidad de que cada etiqueta sea el estado inicial dado por la f√≥rmula:\n",
        "    - **`pi[s]`**: Probabilidad inicial de la etiqueta.\n",
        "    - **`B[s, obs_idx[0]]`**: Probabilidad de la etiqueta **`s`** dada la primera palabra de la oraci√≥n (en su √≠ndice).\n",
        "- **`psi[0, s] = 0`**: No se necesita un backpointer en el primer paso porque no hay un paso anterior."
      ],
      "metadata": {
        "id": "MoaBz3qu_Dfo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IahyeGn2_Hs9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}