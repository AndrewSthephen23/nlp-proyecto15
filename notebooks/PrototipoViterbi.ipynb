{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6d011e",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏èLos Tres Problemas Fundamentales de los HMM\n",
    "\n",
    "### 1. **Problema de Evaluaci√≥n (Evaluaci√≥n de la probabilidad de las observaciones)**\n",
    "\n",
    "**Descripci√≥n del problema:**\n",
    "\n",
    "Dada una secuencia de observaciones $O = (O_1, O_2, ..., O_T)$ y un modelo $\\lambda = (A, B, \\pi)$, el **Problema de Evaluaci√≥n** consiste en calcular la probabilidad $P(O | \\lambda)$, que es la probabilidad de observar la secuencia $O$ bajo el modelo HMM $\\lambda$, es decir:\n",
    "\n",
    "$P(O | \\lambda) = \\sum_{Q} P(O, Q | \\lambda)$\n",
    "\n",
    "Donde $Q = (q_1, q_2, ..., q_T)$ representa una secuencia de estados ocultos (etiquetas POS).\n",
    "\n",
    "**Soluci√≥n:**\n",
    "\n",
    "Este problema se resuelve utilizando el **algoritmo Forward** o el **algoritmo Backward**. Ambos m√©todos calculan la probabilidad de las observaciones considerando todas las posibles secuencias de estados, pero sin tratar de encontrar la secuencia m√°s probable.\n",
    "\n",
    "- **Forward Algorithm**: Se usa para calcular la probabilidad de observar una secuencia de palabras dado un modelo HMM. Trabaja recursivamente, sumando probabilidades de los estados previos y las observaciones.\n",
    "- **Backward Algorithm**: Calcula la probabilidad de observar la secuencia de observaciones a partir de un punto espec√≠fico en el tiempo hacia atr√°s.\n",
    "\n",
    "**En resumen:** Este es el **problema de calcular la probabilidad de las observaciones** dados los par√°metros del modelo. El algoritmo de Forward/Backward lo resuelve eficientemente sin tener que recorrer todas las secuencias posibles.\n",
    "\n",
    "### 2. **Problema de Decodificaci√≥n (Encontrar la secuencia de estados m√°s probable)**\n",
    "\n",
    "**Descripci√≥n del problema:**\n",
    "\n",
    "Dada una secuencia de observaciones $O = (O_1, O_2, ..., O_T)$ y un modelo $\\lambda = (A, B, \\pi)$, el **Problema de Decodificaci√≥n** consiste en encontrar la secuencia de estados ocultos $Q = (q_1, q_2, ..., q_T)$ que tiene la **mayor probabilidad de haber generado las observaciones**:\n",
    "\n",
    "$Q = \\arg \\max_{Q} P(Q | O, \\lambda)$\n",
    "\n",
    "Es decir, nos interesa encontrar la **mejor secuencia de etiquetas POS** dada una secuencia de palabras.\n",
    "\n",
    "**Soluci√≥n:**\n",
    "\n",
    "Este problema se resuelve utilizando el **algoritmo de Viterbi**, que es un **algoritmo de programaci√≥n din√°mica**. Viterbi busca la secuencia de estados m√°s probable, dada la secuencia de observaciones.\n",
    "\n",
    "La idea es almacenar las **probabilidades parciales** y usar las probabilidades de transici√≥n y emisi√≥n para encontrar la secuencia √≥ptima de estados, es decir, la mejor secuencia de etiquetas POS que explica las palabras.\n",
    "\n",
    "**En resumen:** Este es el **problema de encontrar la secuencia m√°s probable de etiquetas** para un conjunto de observaciones. El algoritmo de Viterbi resuelve este problema de manera eficiente, comparando todas las posibles secuencias y manteniendo solo la m√°s probable.\n",
    "\n",
    "### 3. **Problema de Aprendizaje (Estimaci√≥n de los par√°metros del modelo)**\n",
    "\n",
    "**Descripci√≥n del problema:**\n",
    "\n",
    "Este problema se presenta cuando se tiene un conjunto de **observaciones etiquetadas** (o no) y se quiere **ajustar** los par√°metros del modelo HMM, es decir, calcular $\\lambda = (A, B, \\pi)$ de tal manera que el modelo ajuste lo mejor posible a los datos observados.\n",
    "\n",
    "Esto incluye **estimaciones de probabilidades de transici√≥n (A)**, probabilidades de emisi√≥n (B), y probabilidades iniciales de los estados (œÄ).\n",
    "\n",
    "**Soluci√≥n:**\n",
    "\n",
    "El **Problema de Aprendizaje** se resuelve con el **algoritmo de Baum-Welch**, que es una forma del **algoritmo Expectation-Maximization (EM)**. Este algoritmo ajusta iterativamente los par√°metros $A$, $B$  y $\\pi$ para maximizar la probabilidad de los datos observados.\n",
    "\n",
    "**En resumen:** Este es el **problema de estimar los par√°metros del modelo HMM** a partir de datos observados. El algoritmo de Baum-Welch permite ajustar esos par√°metros sin necesidad de conocer las secuencias de estados, lo cual es √∫til para el aprendizaje autom√°tico.\n",
    "\n",
    "---\n",
    "\n",
    "### üßë‚Äçüíª **Enfoque en el Problema de Decodificaci√≥n usando Viterbi:**\n",
    "\n",
    "Dado que estamos trabajando en un **problema de decodificaci√≥n** para etiquetar una secuencia de palabras con sus correspondientes etiquetas POS, **el algoritmo de Viterbi** es el que m√°s nos interesa.\n",
    "\n",
    "- En este caso, no necesitamos calcular las probabilidades de todas las secuencias de estados (como en el problema de evaluaci√≥n), sino que queremos **encontrar la secuencia de estados m√°s probable** que genera nuestra secuencia de observaciones (palabras)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed09b7b",
   "metadata": {},
   "source": [
    "# ‚ú®Algoritmo de Viterbi\n",
    "\n",
    "El algoritmo de Viterbi es un algoritmo de programaci√≥n din√°mica que encuentra la secuencia de estados ocultos m√°s probable (la \"ruta de Viterbi\") que resulta en una secuencia de observaciones dada en un HMM.\n",
    "\n",
    "Los pasos del algoritmo de Viterbi son los siguientes:\n",
    "\n",
    "### **1. Inicializaci√≥n (t = 1)**\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Para cada estado $S_i \\in S$ (en nuestro caso, las etiquetas POS como NOUN, VERB, etc.), calculamos la probabilidad de la secuencia m√°s probable hasta el primer estado dado la primera observaci√≥n.\n",
    "\n",
    "**F√≥rmula:**\n",
    "\n",
    "$\\delta_1(i) = \\pi_i \\cdot b_i(O_1)$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\delta_t(i)$ es la probabilidad de la secuencia m√°s probable hasta el tiempo $t$ que termina en el estado $S_i$ (en el paso 1, es solo el primer estado).\n",
    "- $\\pi_i$ es la probabilidad inicial de estar en el estado $S_i$ al inicio de la secuencia (de la matriz $\\pi$).\n",
    "- $b_i(O_1)$ es la probabilidad de emitir la primera observaci√≥n $O_1$ (en nuestro caso la primera palabra, \"Time\"), dado que estamos en el estado $S_i$ (de la matriz de emisi√≥n $B$).\n",
    "\n",
    "**Matriz de Backpointers ($\\psi_1(i)$):**\n",
    "\n",
    "- Para $t=1$, inicializamos la matriz de backpointers $\\psi_1(i)$ con valores nulos, ya que no tenemos un estado previo para el primer paso.\n",
    "\n",
    "### **2. Recursi√≥n (t = 2 hasta T)**\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Para cada tiempo $t$  desde 2 hasta $T$ (la longitud de la secuencia de observaciones), y para cada estado $S_j \\in S$, calculamos la probabilidad de llegar al estado $S_j$ en el tiempo $t$ dado cualquier estado $S_i$ en el tiempo $t‚àí1$.\n",
    "\n",
    "**F√≥rmula:**\n",
    "\n",
    "$\\delta_t(j) = \\max_{1 \\leq i \\leq N} \\left[ \\delta_{t-1}(i) \\cdot a_{ij} \\right] \\cdot b_j(O_t)$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\delta_t(j)$ es la probabilidad m√°xima de llegar al estado $S_j$ en el tiempo $t$, viniendo de cualquier estado $S_i$ en el tiempo $t‚àí1$, multiplicado por la probabilidad de observar $O_t$ desde el estado $S_j$.\n",
    "- $a_{ij}$ es la probabilidad de transici√≥n de $S_i$ a $S_j$ (de la matriz $A$).\n",
    "- $b_j(O_t)$ es la probabilidad de emitir la observaci√≥n $O_t$ (la palabra en ese paso) dado que estamos en el estado $S_j$ (de la matriz $B$).\n",
    "\n",
    "**Matriz de Backpointers ($\\psi_t(j)$):**\n",
    "\n",
    "- El backpointer $\\psi_t(j)$ almacena el estado $S_i$ que gener√≥ la probabilidad m√°xima para $\\delta_t(j)$. Es decir:\n",
    "\n",
    "$\\psi_t(j) = \\arg\\max_{1 \\leq i \\leq N} \\left[ \\delta_{t-1}(i) \\cdot a_{ij} \\right]$\n",
    "\n",
    "Esto nos permite reconstruir la secuencia de estados m√°s probable al final del algoritmo.\n",
    "\n",
    "### **3. Terminaci√≥n (t = T)**\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Determinar la probabilidad de la secuencia de estados ocultos m√°s probable y el √∫ltimo estado de esa secuencia.\n",
    "\n",
    "**F√≥rmula:**\n",
    "\n",
    "La probabilidad de la secuencia de estados ocultos m√°s probable es:\n",
    "\n",
    "$P^* = \\max_{1 \\leq i \\leq N} [\\delta_T(i)]$\n",
    "\n",
    "Donde $\\delta_T(i)$ es la probabilidad m√°xima de que la secuencia de estados m√°s probable termine en el estado $S_i$ en el √∫ltimo paso $T$.\n",
    "\n",
    "El √∫ltimo estado de la secuencia de estados m√°s probable es:\n",
    "\n",
    "$q_T^* = \\arg\\max_{1 \\leq i \\leq N} [\\delta_T(i)]$\n",
    "\n",
    "### **4. Retroceso (t = T-1 hasta 1)**\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Recuperar la secuencia de estados ocultos m√°s probable utilizando los backpointers, empezando desde el √∫ltimo estado $q_T^*$ y retrocediendo.\n",
    "\n",
    "**F√≥rmula:**\n",
    "\n",
    "Reconstituimos la secuencia de estados ocultos m√°s probable utilizando los backpointers:\n",
    "\n",
    "$q_t^* = \\psi_{t+1}(q_{t+1}^*)$\n",
    "\n",
    "Para $t = T-1, T-2, ..., 1$.\n",
    "\n",
    "Esto nos da la secuencia de etiquetas m√°s probable de los estados ocultos que generaron las observaciones dadas.\n",
    "\n",
    "***üßë‚Äçüè´Implementaci√≥n paso a paso con el caso de POS-tagging***\n",
    "Ahora que entendemos los pasos te√≥ricos del **algoritmo de Viterbi**, lo siguiente ser√° usar los **valores de œÄ, A y B** (las probabilidades iniciales, de transici√≥n y de emisi√≥n) que definimos antes, y aplicar este algoritmo a la secuencia de palabras \"Time flies like an arrow\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f394bdc",
   "metadata": {},
   "source": [
    "##  üëü **Paso 1: Inicializaci√≥n**\n",
    "\n",
    "Primero, debemos crear las matrices **Œ¥ (delta)** y **œà (backpointers)**. La matriz **Œ¥** se usar√° para almacenar las probabilidades de la secuencia m√°s probable hasta cada estado, y **œà** para almacenar los backpointers.\n",
    "\n",
    "### **Datos que necesitamos:**\n",
    "\n",
    "- Las probabilidades iniciales $\\pi$, la matriz de transici√≥n $A$ y la matriz de emisi√≥n $B$, que ya definimos anteriormente.\n",
    "\n",
    "### C√≥digo para la inicializaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22345665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz Œ¥ despu√©s de la inicializaci√≥n:\n",
      "[0.12 0.03 0.   0.  ]\n",
      "\n",
      "Matriz de backpointers œà despu√©s de la inicializaci√≥n:\n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Datos de entrada (observaciones y estados)\n",
    "sentence = [\"Time\", \"flies\", \"like\", \"an\", \"arrow\"]\n",
    "states = [\"NOUN\", \"VERB\", \"DET\", \"PREP\"]\n",
    "observations = sentence\n",
    "\n",
    "# √çndices para facilitar el acceso\n",
    "state_idx = {s: i for i, s in enumerate(states)}\n",
    "obs_idx = {w: i for i, w in enumerate(observations)}\n",
    "\n",
    "# Probabilidades iniciales (œÄ), transiciones (A) y emisiones (B)\n",
    "pi = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "A = np.array([\n",
    "    [0.1, 0.6, 0.2, 0.1],  # desde NOUN\n",
    "    [0.3, 0.1, 0.1, 0.5],  # desde VERB\n",
    "    [0.7, 0.1, 0.1, 0.1],  # desde DET\n",
    "    [0.6, 0.2, 0.1, 0.1],  # desde PREP\n",
    "])\n",
    "B = np.array([\n",
    "    [0.3, 0.2, 0.1, 0.0, 0.4],  # NOUN\n",
    "    [0.1, 0.6, 0.2, 0.0, 0.1],  # VERB\n",
    "    [0.0, 0.0, 0.0, 1.0, 0.0],  # DET\n",
    "    [0.0, 0.0, 0.9, 0.1, 0.0],  # PREP\n",
    "])\n",
    "\n",
    "# N√∫mero de observaciones y estados\n",
    "T = len(observations)\n",
    "N = len(states)\n",
    "\n",
    "# Inicializamos las matrices Œ¥ y œà\n",
    "delta = np.zeros((T, N))  # Probabilidades\n",
    "psi = np.zeros((T, N), dtype=int)  # Backpointers\n",
    "\n",
    "# Paso 1: Inicializaci√≥n (t = 1)\n",
    "for s in range(N):\n",
    "    delta[0, s] = pi[s] * B[s, obs_idx[observations[0]]]\n",
    "    psi[0, s] = 0  # No hay backpointer al inicio, pero lo necesitamos para el bucle\n",
    "\n",
    "# Mostrar la inicializaci√≥n\n",
    "print(\"Matriz Œ¥ despu√©s de la inicializaci√≥n:\")\n",
    "print(delta[0])\n",
    "print(\"\\nMatriz de backpointers œà despu√©s de la inicializaci√≥n:\")\n",
    "print(psi[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb0bfb",
   "metadata": {},
   "source": [
    "### **Explicaci√≥n del c√≥digo:**\n",
    "\n",
    "1. **√çndices**: Utilizamos dos diccionarios (`state_idx` y `obs_idx`) para mapear estados y observaciones a √≠ndices. Esto facilita el acceso a los valores en las matrices $A$ y $B$.\n",
    "2. **Matriz $\\delta$**: Al principio (cuando $t=1$), calculamos la probabilidad de cada estado $S_i$ como:\n",
    "    \n",
    "    $\\delta_1(i) = \\pi_i \\cdot b_i(O_1)$\n",
    "    \n",
    "    Esto corresponde a la probabilidad de que el primer estado sea $S_i$ dado que hemos observado la palabra $O_1$ (en este caso, \"Time\").\n",
    "    \n",
    "3. **Matriz $\\psi$**: Los **backpointers** para $t = 1$ se inicializan como cero, ya que no hay estados previos desde los que llegamos.\n",
    "\n",
    "## **üëüPaso 2: Recursi√≥n (t = 2 hasta T)**\n",
    "\n",
    "Aqu√≠ es donde vamos a hacer los c√°lculos para cada paso de la secuencia de observaciones.\n",
    "\n",
    "### C√≥digo para la recursi√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bd0835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz Œ¥ despu√©s de la recursi√≥n:\n",
      "[[1.2000e-01 3.0000e-02 0.0000e+00 0.0000e+00]\n",
      " [2.4000e-03 4.3200e-02 0.0000e+00 0.0000e+00]\n",
      " [1.2960e-03 8.6400e-04 0.0000e+00 1.9440e-02]\n",
      " [0.0000e+00 0.0000e+00 1.9440e-03 1.9440e-04]\n",
      " [5.4432e-04 1.9440e-05 0.0000e+00 0.0000e+00]]\n",
      "\n",
      "Matriz de backpointers œà despu√©s de la recursi√≥n:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 1 0 1]\n",
      " [0 0 3 3]\n",
      " [2 2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Recursi√≥n (t = 2 hasta T)\n",
    "for t in range(1, T):\n",
    "    for s in range(N):\n",
    "        prob = delta[t-1] * A[:, s] * B[s, obs_idx[observations[t]]]\n",
    "        delta[t, s] = np.max(prob)\n",
    "        psi[t, s] = np.argmax(prob)\n",
    "\n",
    "# Mostrar la matriz Œ¥ despu√©s de la recursi√≥n\n",
    "print(\"\\nMatriz Œ¥ despu√©s de la recursi√≥n:\")\n",
    "print(delta)\n",
    "print(\"\\nMatriz de backpointers œà despu√©s de la recursi√≥n:\")\n",
    "print(psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c141d8",
   "metadata": {},
   "source": [
    "### **Explicaci√≥n del c√≥digo:**\n",
    "\n",
    "- En cada paso $t$, calculamos la probabilidad m√°xima $\\delta_t(j)$ para cada estado $S_j$, viniendo de cualquier estado $S_i$  en el paso anterior, y multiplicado por la probabilidad de emitir la observaci√≥n $O_t$.\n",
    "- Usamos **numpy** para calcular estas probabilidades de forma vectorizada y encontrar la probabilidad m√°xima.\n",
    "\n",
    "##  **üëüPaso 3: Terminaci√≥n (t = T)**\n",
    "\n",
    "Al final de la secuencia, necesitamos encontrar cu√°l es el **estado final m√°s probable** y calcular la probabilidad total.\n",
    "\n",
    "### C√≥digo para la terminaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d49f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilidad de la secuencia m√°s probable: 0.0005443199999999999\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Terminaci√≥n (t = T)\n",
    "best_path = np.zeros(T, dtype=int)\n",
    "best_path[-1] = np.argmax(delta[T-1])\n",
    "\n",
    "# Mostrar la probabilidad m√°xima al final\n",
    "P_star = np.max(delta[T-1])\n",
    "print(f\"\\nProbabilidad de la secuencia m√°s probable: {P_star}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6c551",
   "metadata": {},
   "source": [
    "### **Explicaci√≥n del c√≥digo:**\n",
    "\n",
    "- Aqu√≠ encontramos el **√∫ltimo estado m√°s probable** utilizando el m√°ximo valor de la √∫ltima fila de $\\delta$.\n",
    "- La probabilidad total de la secuencia es el valor m√°ximo en $\\delta_T$.\n",
    "\n",
    "## **üëüPaso 4: Retroceso (t = T-1 hasta 1)**\n",
    "\n",
    "Finalmente, recuperamos la secuencia de estados m√°s probable utilizando los **backpointers**.\n",
    "\n",
    "### C√≥digo para el retroceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe3341ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Secuencia de etiquetas m√°s probable:\n",
      "['NOUN', 'VERB', 'PREP', 'DET', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "# Paso 4: Retroceso (t = T-1 hasta 1)\n",
    "for t in range(T-2, -1, -1):\n",
    "    best_path[t] = psi[t+1, best_path[t+1]]\n",
    "\n",
    "# Mostrar la secuencia de etiquetas m√°s probable\n",
    "best_labels = [states[s] for s in best_path]\n",
    "print(\"\\nSecuencia de etiquetas m√°s probable:\")\n",
    "print(best_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fa6dc",
   "metadata": {},
   "source": [
    "### **Explicaci√≥n del c√≥digo:**\n",
    "\n",
    "- Retrocedemos desde el √∫ltimo estado m√°s probable, utilizando los **backpointers** almacenados en la matriz $\\psi$.\n",
    "- La secuencia final de etiquetas corresponde a los estados m√°s probables para cada observaci√≥n.\n",
    "\n",
    "La salida:\n",
    "\n",
    "```python\n",
    "['NOUN', 'VERB', 'PREP', 'DET', 'NOUN']\n",
    "```\n",
    "\n",
    "es **coherente, l√≥gica y esperable**, tanto **ling√º√≠stica como estad√≠sticamente**, y demuestra que el **algoritmo de Viterbi est√° funcionando correctamente** con el modelo HMM que construimos.\n",
    "\n",
    "### üîç ¬øC√≥mo interpretar este resultado?\n",
    "\n",
    "Aplicado a la oraci√≥n:\n",
    "\n",
    "```python\n",
    "Time     flies     like     an     arrow\n",
    "NOUN     VERB      PREP     DET     NOUN\n",
    "```\n",
    "\n",
    "Esto sugiere que:\n",
    "\n",
    "- `\"Time\"` es un **sustantivo** (NOUN),\n",
    "- `\"flies\"` es un **verbo** (VERB),\n",
    "- `\"like\"` es una **preposici√≥n** (PREP),\n",
    "- `\"an\"` es un **determinante** (DET),\n",
    "- `\"arrow\"` es un **sustantivo** (NOUN).\n",
    "\n",
    "### ‚úÖ ¬øPor qu√© tiene sentido?\n",
    "\n",
    "1. **\"Time flies\"** ‚Üí puede leerse como un sujeto (\"time\") seguido de un verbo (\"flies\").\n",
    "2. **\"like an arrow\"** ‚Üí es una frase preposicional com√∫n (preposici√≥n + determinante + sustantivo).\n",
    "\n",
    "Esto refleja **una interpretaci√≥n sem√°ntica v√°lida**:\n",
    "\n",
    "> \"El tiempo vuela como una flecha.\"\n",
    "\n",
    "Adem√°s, el resultado se alinea con las probabilidades que definimos manualmente:\n",
    "\n",
    "- `\"flies\"` ten√≠a probabilidad alta tanto como VERB como NOUN ‚Üí el contexto ayud√≥ a resolver la ambig√ºedad.\n",
    "- `\"like\"` tambi√©n era ambigua (PREP vs VERB), pero Viterbi prefiri√≥ PREP por su conexi√≥n con ‚Äúan arrow‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e6d5a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
